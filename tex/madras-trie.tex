%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode
\documentclass[]{article}

\usepackage{fontspec}

\usepackage{hyperref}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[C]{Data Indexing$\bullet$ Succinct Tries $\bullet$ Madras trie - A highly space efficient succinct trie adapted for static database indexing}
\fancyfoot[L]{ Licensed under CC 4.0 Int'l Attribution License \copyright 2022-24 Siara Logics (cc)}
\fancyfoot[R]{\thepage}

%opening
\title{Madras trie - A highly space efficient succinct trie adapted for static database indexing}
\author{Arundale Ramanathan}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		Madras trie is a highly space efficient succinct trie data structure adapted for performance and features needed for static transactional data indexing.  This articles explains how this novel data structure performs against other state-of-the-art data structures and how it is can be used for both traditional succinct rank/select dictionaries as well as for static primary and secondary database indexing and keyword indexing.

	\end{abstract}

	\section{Summary}

	Although OLTP (Online Transaction Processing) involves huge amount of data processing by way of insert, update and delete operations on tables, the data becomes static after a while and are invariably unchanged.  While there could be a huge number of changes from the time a transaction is created and finalised, there are usually not much changes once a transaction reaches the 'Confirmed' state or 'Final' state.

	Once such states are reached, these data are heavily used for reporting, reference and analytics and can be considered static for practical purposes.  However they still continue occupy the same amount of space in RDBMS systems as they did when they were being actively modified.

	Such transactional data are eventually archived at a later stage but retrieving from them is not very efficient and so can't be used for reporting, reference or analysis.

	\section{Statement of need}

	Madras trie was initially developed for storing word or phrase dictionaries for the Unishox project.  However it was realised that it could have a huge impact on space efficiency for indexing transactional data.

	Transactional data reach a near static state within hours or days after they are created, but are kept for reporting, reference or analysis for months or years before they are archived on cold storage.  The space occupied by such data including secondary indices occupy several times the size of raw data.  Such data can be archived immediately using Madras trie and accompanying technologies while still be available for reporting, reference and analysis while occupying fraction of the size of raw data.

	Column oriented data formats such as Apache Parquet and ORC are available for storing data and designed with space efficiency in mind, but do not offer much choice by way of data indexing.  These technologies heavily rely on general purpose compression techniques such as LZ4 and ZStnadard.  Often blocks of record would need to be uncompressed and scanned to locate specific set of records.

	\section{Basic Definitions}

	In computer science, a \emph{succinct data structure} is a data structure which uses an amount of space that is "close" to the information-theoretic lower bound, but (unlike other compressed representations) still allows for efficient query operations. The concept was originally introduced by Jacobson \cite{1} to encode bit vectors, (unlabeled) trees, and planar graphs. Unlike general lossless data compression algorithms, succinct data structures retain the ability to use them in-place, without decompressing them first.  \cite{2}.

	One of the main types of entropy coding is about creating and assigning a unique \emph{prefix-free code} to each unique symbol that occurs in the input. These entropy encoders then compress data by replacing each fixed-length input symbol with the corresponding variable-length prefix-free output code word.
	
	According to Shannon's source coding theorem, the optimal code length for a symbol is $-log_bP$, where b is the number of symbols used to make output codes and P is the probability of the input symbol \cite{2}. Therefore, the most common symbols use the shortest codes.
	
	\section{Background}

	\section{This research}

	A hybrid encoding method is proposed relying on the three encoding techniques \emph{viz.} Entropy encoding, Dictionary coding and Delta encoding methods for optimal compression.
	
	While existing techniques focus on either Unicode character sequences or only English characters, Unishox uses multiple techniques to achieve the best compression ratio all round.
	
	For Unicode, Delta encoding is proposed because usually the difference between subsequent symbols is quite less while encoding text of a particular language. SCSU is slightly better with switching windows, but overall it was found that plain Delta coding works well considering that usually there is only one language text to be compressed and some languages span a lot of windows.
	
	SCSU and BOCU do have special features for Unicode that Unishox does not address such as dynamic windows, binary order maintenance, XML suitability and MIME friendliness. Unishox uses plain delta encoding to achieve the best compression.
	
	For English letters, unlike shoco, a fixed frequency table is proposed, generated based on the characterestics of English language letter frequency. The research carried out by Oxford University \cite{7} and other sources \cite{7} \cite{9} have been used to arrive at a unique method that takes advantage of the conventions of the language.
	
	A single fixed model is used because of the advantages it offers over the training models of shoco. The disadvantage with the training model, although it may appear to offer more compression, is that it does not consider the patterns that usually appear during text formation. It can be seen that this performs better than pre-trained model of shoco (See performance section).
	
	Unlike smaz and shoco, no \emph{a priori} knowledge about the input text is assumed. However knowledge from the research carried out on the language and common patterns of sentence formation have been used to come out with pre-assigned codes for each letter.
	
	\section{Model}
	
	In the ASCII chart, we have 95 printable letters starting from 32 through 126. For the purpose of arriving at fixed codes for each of these letters, two sets of prefix-free codes are used.
	
	The first set consists of 28 codes, which are: 00, 010, 011, 1000, 1001, 1010, 1011, 1100, 11010, 11011, 111000, 111001, 111010, 1110110, 1110111, 1111000, 1111001, 1111010, 11110110, 11110111, 11111000, 11111001, 11111010, 11111011, 11111100, 11111101, 11111110, 11111111. These are called vertical codes (vcodes).
	
	The second set consists of 5 codes, which by default will be 00, 01, 10, 110, 111. These are called horizontal codes (hcodes). These 5 codes can be configured according to the composition of text that needs to be compressed.
	
	With these two sets of codes, several sets of letters are formed as shown in the table below and some rules are formed based on how patterns appear in short strings.
	
	\begin{center}
		\begin{tabular}{ | l | c | c | c | c | c | c | } \hline
			\textbf{hcode $\rightarrow$} & \textbf{00} & \textbf{01} & \textbf{10} & \textbf{110} & \textbf{111} \\ \hline
			\multirow{2}{*}{\textbf{$\downarrow$ vcode}} & \textbf{Set 1}& \textbf{Set 2} & \textbf{Set 3} & \textbf{Set 4} & \textbf{Set 5} \\
			& \textbf{Alpha} & \textbf{Sym} & \textbf{Num} & \textbf{Dictionary} & \textbf{Delta} \\ \hline
			\textbf{00} & switch & \textquotedblleft & switch &
			\multirow{27}{*}
			{\makecell{$<$length$>$\\$<$distance$>$}} & 
			\multirow{27}{*}
			{\makecell{$<$code$>$\\
					$<$sign$>$\\$<$delta$>$}}
			\\ \cline{1-4}
			\textbf{010} & sp & \{ & , & & \\ \cline{1-4}
			\textbf{011} & e / E & \} & . & & \\ \cline{1-4}
			\textbf{1000} & t / T & \_ & 0 & & \\ \cline{1-4}
			\textbf{1001} & a / A & $<$ & 1 & & \\ \cline{1-4}
			\textbf{1010} & o / O & $>$ & 9 & & \\ \cline{1-4}
			\textbf{1011} & i / I & : & 2 & & \\ \cline{1-4}
			\textbf{1100} & n / N & lf & 5 & & \\ \cline{1-4}
			\textbf{11010} & s / S & crlf & - & & \\ \cline{1-4}
			\textbf{11011} & r / R & [ & / & & \\ \cline{1-4}
			\textbf{111000} & l / L & ] & 3 & & \\ \cline{1-4}
			\textbf{111001} & c / C & \textbackslash & 4 & & \\ \cline{1-4}
			\textbf{111010} & d / D & ; & 6 & & \\ \cline{1-4}
			\textbf{1110110} & h / H & ' & 7 & & \\ \cline{1-4}
			\textbf{1110111} & u / U & tab & 8 & & \\ \cline{1-4}
			\textbf{1111000} & p / P & @ & ( & & \\ \cline{1-4}
			\textbf{1111001} & m / M & * & ) & & \\ \cline{1-4}
			\textbf{1111010} & b / B & \& & sp & & \\ \cline{1-4}
			\textbf{11110110} & g / G & ? & = & & \\ \cline{1-4}
			\textbf{11110111} & w / W & ! & + & & \\ \cline{1-4}
			\textbf{11111000} & f / F & \^{} & \$ & & \\ \cline{1-4}
			\textbf{11111001} & y / Y & $|$ & \% & & \\ \cline{1-4}
			\textbf{11111010} & v / V & cr & \# & & \\ \cline{1-4}
			\textbf{11111011} & k / K & ~ & seq4 & & \\ \cline{1-4}
			\textbf{11111100} & q / Q & ` & seq5 & & \\ \cline{1-4}
			\textbf{11111101} & j / J & seq1 & seq6 & & \\ \cline{1-4}
			\textbf{11111110} & x / X & seq2 & rpt & & \\ \cline{1-4}
			\textbf{11111111} & z / Z & seq3 & term & & \\ \hline
		\end{tabular}
	\end{center}
	
	\section{Rules}
	
	\subsection{Basic rules}
	\begin{itemize}
		\item[$\bullet$] It can be seen that the more frequent symbols are assigned smaller codes.
		\item[$\bullet$] Set 1 is always active when beginning compression. So the letter \emph{e} has the code 011, \emph{t} 1010 and so on.
	\end{itemize}
	
	\subsection{Upper case symbols}
	\begin{itemize}
		\item[$\bullet$] For encoding uppercase letters, the switch symbol is used followed by 00 and the code against the symbol itself. For example, E is encoded as 00 00 011.
		\item[$\bullet$] If uppercase letters appear continuously, then the encoder may decide to switch to upper case using the prefix 00 00 00 00. After that, the same codes for lower case are used to indicate upper case letters until the code sequence 00 00 is used again to return to lower case.
	\end{itemize}
	
	\subsection{Numbers and related symbols}
	\begin{itemize}
		\item[$\bullet$] Symbols in Set 2 are encoded by first switching to the set by using 00 followed by 01. So the symbol " is encoded as 00 01 00.
		\item[$\bullet$] Numbers in Set 3 are encoded by first switching to the set by using 00 followed by 10. So the symbol 9 is encoded as 00 10 1010.
		\item[$\bullet$] For Set 3, whenever a switch is made from Set 1 to any number (0 to 9), it makes Set 3 active. So subsequent numbers symbols in Set 3 can be encoded without the switch symbol, as in 111000 for 3, 111001 for 4 and so on.
		\item[$\bullet$] To return to Set 1 in this case, the code 0000 is used.
		\item[$\bullet$] However, when other symbols in Set 3 are encoded from Set 1, Set 3 is not made active.
	\end{itemize}
	
	\subsection{Sticky sets}
	\begin{itemize}
		\item[$\bullet$] When switching to Set 3 for encoding numbers (0-9), it becomes active and is said to be sticky till Set 1 is made active using the symbol 0000.
		\item[$\bullet$] Encoding Upper case symbols become sticky when switching using 0000 0000.
		\item[$\bullet$] Encoding Unicode symbols become sticky when switching using 0000 010, as seen in a subsequent section.
		\item[$\bullet$] However, no other set is sticky. Set 1 is default. Set 3 automatically becomes sticky when any numeral is encoded and Upper case letters can be made sticky by using 00000000.
		\item[$\bullet$] Symbols in Set 2 are never sticky. Once encoded the previous sticky set becomes active.
	\end{itemize}
	
	\subsection{Special symbols}
	\begin{itemize}
		\item[$\bullet$] term in Set 3 indicates termination of encoding. This is used if length of the encoded string is not available. In case the length of encoded string is available, term symbol need not be encoded and encoding can stop with the last symbol encoded. However, the first part of the term symbol needs to be encoded in the last byte after the bits for the last symbol. Further if Unicode set is sticky and active, first it needs to be exited using the exit sequence 11111 00 and then the term symbol should be encoded.
		\item[$\bullet$] rpt in Set 3 indicates that the symbol last encoded is to be repeated specified number of times.
		\item[$\bullet$] CRLF in Set 2 is encoded using a single code. It will be expanded as two bytes CR LF. If only LF is used, such as in Unix like systems, a separate code is used in Set 2. Also, in the rare case that only CR appears, another code is provided in Set 2.
	\end{itemize}
	
	\subsection{Repeating letters}
	\begin{itemize}
		\item[$\bullet$] If any letter repeats more than 3 times, a special code (rpt) is used as shown in Set 3 of the model.
		\item[$\bullet$] The encoder first codes the letter using the above codes. Then the rpt code is used followed by the number of times the letter repeats.
		\item[$\bullet$] The number of times the letter repeats is coded using a special bit sequence as explained in section "Encoding counts" that follows.
	\end{itemize}
	
	\subsection{Repeating sections}
	\begin{itemize}
		\item[$\bullet$] If a section repeats, the switch code (00) and another horizontal code (110) is used followed by two fields as described next.
		\item[$\bullet$] The first field indicates the length of the section that repeats.
		\item[$\bullet$] The second field indicates the distance of the repeating section. The distance is counted from the current position.
		\item[$\bullet$] The optional third field is coded only if an array of text is encoded. It is a number indicating the index of the array that the section belongs. If only one text is encoded, then this field is not included. 
		\item[$\bullet$] The first, second and third fields are encoded as explained in the following section "Encoding counts".
	\end{itemize}
	
	\subsection{Encoding Counts}
	\begin{itemize}
		\item[$\bullet$] For encoding counts such as length and distance, five codes are used: 0, 10, 110, 1110, 1111, each code indicating how many bits will follow to indicate count.
		\item[$\bullet$] If code is 00, 2 bits would follow, that is, count is between 0 and 3.
		\item[$\bullet$] If code is 10, 4 bits would follow, that is, count is between 4 and 19.
		\item[$\bullet$] If code is 10, 7 bits would follow, that is, count is between 20 and 147.
		\item[$\bullet$] If code is 110, 11 bits would follow, that is, count is between 148 and 2195.
		\item[$\bullet$] If code is 111, 16 bits would follow, that is, count is between 2196 and 67732.
		\item[$\bullet$] This is shown in tabular form below
		\item[] \begin{tabular}{ | l | l | l |} \hline
			\textbf{Code} & \textbf{Range} & \textbf{Number of bits} \\ \hline
			00 & 0 to 3 & 2 \\ \hline
			10 & 4 to 19 & 5 \\ \hline
			10 & 20 to 147 & 7 \\ \hline
			110 & 148 to 2195 & 11 \\ \hline
			111 & 2196 to 67732 & 16 \\ \hline
		\end{tabular}
	\end{itemize}
	
	\subsection{Encoding Unicode characters}
	\begin{itemize}
		\item[$\bullet$] The switch code 00 followed by 111 is used as prefix to indicate that a Unicode character is being encoded.
		\item[$\bullet$] First, the unicode number is decoded from the input source depending on how it was encoded, such as UTF-8 or UTF-16.
		\item[$\bullet$] For the first unicode character, the number decoded is re-coded to the output as it is, using 00 111 followed by a code as shown in the table below followed by a sign bit 0 (positive), followed by given number of bits shown in the table, depending on the range that the code belongs.
		\item[] \begin{tabular}{ | l | l | l |} \hline
			\textbf{Code} & \textbf{Range} & \textbf{Number of bits} \\ \hline
			00 & 0 to 63 & 6 \\ \hline
			01 & 64 to 4159 & 12 \\ \hline
			100 & 4160 to 20543 & 14 \\ \hline
			101 & 20544 to 86079 & 16 \\ \hline
			110 & 86080 to 2183231 & 21 \\ \hline
			111 & Special code & - \\ \hline
		\end{tabular}
		\item[$\bullet$] The Special code is explained in the next section.
		\item[$\bullet$] For subsequent unicode characters, only the difference between the previous character is re-coded to the output, using sign bit as 1 if the difference is negative. Thus, here, delta coding is used.
		\item[$\bullet$] After 00 111, one of the above codes is used, followed by the sign bit. The sign bit is a single bit. 1 indicates that the number following is negative and 0 indicates that the number following is positive.
		\item[$\bullet$] After the sign bit, the unicode value (or difference) is encoded as a number. The number of bits used depends on the range, as shown in the above table.
		\item[$\bullet$] After encoding the unicode number, the state returns to Set 1, or whichever set was active earlier, unless continuous unicode encoding was started. This is explained in the next section.
	\end{itemize}
	
	\subsection{Encoding continuous Unicode characters}
	\begin{itemize}
		\item[$\bullet$] Since the prefix 00 110 may become an overhead when several Unicode are to be encoded contigously, a continuous unicode encoding code is used (0000 010).
		\item[$\bullet$] After 0000 010 is encoded, unicode characters are encoded continously using delta encoding, until an English character is encountered. When this happens, state is returned to Set 1 using the Special code 11111 00 in the table shown in previous section is used.
		\item[$\bullet$] The Special codes are used only when Unicode characters are coded continuously, to indicate special characters and situations occuring in-between. What follows the Special code 11111 is indicated using the table below:
		\item[] \begin{tabular}{ | l | l |} \hline
			\textbf{Code} & \textbf{Character/Situation} \\ \hline
			0 & Space character \\ \hline
			10 & Switch \\ \hline
			110 & Comma (,) \\ \hline
			1110 & Full stop (.) \\ \hline
			1111 & Line feed (LF) \\ \hline
		\end{tabular}
		\item[$\bullet$] It is found that the above characters appear frequently in between continous Unicode characters and so Special codes are needed to avoid switching back and forth from Set 2.
		\item[$\bullet$] Other symbols in Set 2 or Set 3 can also be encoded within continuous Delta encoding mode using the Switch Code in the above table.
	\end{itemize}
	
	\subsection{Multi way access for Set 2}
	\begin{itemize}
		\item[$\bullet$] Set 2 can be accessed regardless of which set is active, such as Set 1, Set 3, Continuous delta coding or even when continuous Upper case is active. This is because the symbols occur commonly in both Set 1 and 3 and Unicode symbol sequences.
		\item[$\bullet$] For the same reason, the space symbol appears both in Set 1 and Set 3.
	\end{itemize}
	
	\subsection{Encoding punctuations}
	\begin{itemize}
		\item[$\bullet$] Some languages, such as Japanese and Chinese use their own punctuation characters. For example full-stop is indicated using U+3002 which is represented visually as a small circle.
		\item[$\bullet$] Encoding such special full-stops were supported in the earlier version of Unishox for better compression. However since this was leading to confusion and ambiguity, any special treatment for such punctuations are excluded in the present version of Unishox (2) and this is left to delta coding. It also does not make much difference in compression ratio.
	\end{itemize}
	
	\subsection{Common templates}
	\begin{itemize}
		\item[$\bullet$] Some special templates are known to occur frequently and are encoded using 00 10 00 followed the codes mentioned in the table below.
		\item[] 
		\begin{tabular}{ | l | l |} \hline
			\textbf{Code} & \textbf{Situation} \\ \hline
			00 & Template for date, time and phone numbers \\ \hline
			01 & Hex nibbles lower case \\ \hline
			100 & Hex GUID lower case \\ \hline
			101 & Hex nibbles upper case \\ \hline
			110 & Hex GUID upper case \\ \hline
			111 & Binary (ASCII 0-31, 128-255) \\ \hline
		\end{tabular}
		\item[$\bullet$] The code 0 indicates that one of the codes for Date, Time or Phone number follows, which is encoded according to the following table:
		\item[] 
		\begin{tabular}{ | l | l | l |} \hline
			\textbf{Code} & \textbf{Description} & \textbf{Template} \\ \hline
			00 & Standard ISO timestamp & tfff-of-tfTtf:rf:rf.fffZ \\ \hline
			01 & Date only & tfff-of-tf \\ \hline
			10 & US Phone number & (fff) fff-ffff \\ \hline
			110 & Time only & tf:rf:rf \\ \hline
			111 & Reserved \\ \hline
		\end{tabular}
		\item[]Partial matches of the template can also be encoded using this. For example, the string "2021-07-15T20:00:00" can be compressed using above template by specifying how many characters of the template are unused the end. In this case 5 characters are unused.
		\item[]The encoding sequence would be: 00 10 00 0 <template code> <number of unused letters> <filled template>. The method described in "Encoding counts" section is used to encode <number of unused letters>.
		\item[]In the template, following are the codes used and the size occupied in bits. Since fewer bits are sufficient to represent a number, it results in lot of savings.
		\item[]\begin{tabular}{ | l | l | l |} \hline
			\textbf{Letter} & \textbf{Bits} & \textbf{Range} \\ \hline
			o & 1 & 0 to 1 \\ \hline
			t & 2 & 0 to 3 \\ \hline
			r & 3 & 0 to 7 \\ \hline
			f & 4 & 0 to F \\ \hline
		\end{tabular}
		\item[]Using this method, the ISO timestamp which is 24 bytes in length compresses to only 9 bytes.
		\item[]For example, "2021-07-15T16:37:35" would be encoded as 00 10 00 0 0 10 0001 10 0000 0010 0001 0 0111 01 0101 01 0110 011 0111 011 0101. The codes are explained in the table below:
		\item[]\begin{tabular}{ | l | l |} \hline
			\textbf{Code} & \textbf{Description} \\ \hline
			00 10 00 & Code for common templates \\ \hline
			0 & Code for string template \\ \hline
			0 & Template used (tfff-of-tfTtf:rf:rf.fffZ) \\ \hline
			10 0001 & Encode count 5 unused at the end \\ \hline
			10 0000 0010 0001 & 2021 \\ \hline
			0 0111 & 07 \\ \hline
			01 0101 & 15 \\ \hline
			01 0110 & 16 \\ \hline
			011 0111 & 37 \\ \hline
			011 0101 & 35 \\ \hline
		\end{tabular}
		\item[$\bullet$]The codes 10 and 1110 are used to encode a sequence of lower and upper Hex nibbles respectively. 10 or 1110 is followed by the count of nibbles encoded as explained in the "Encoding counts" section. After this, each nibble is encoded using 4 bits each.
		\item[$\bullet$]The code 110 and 11110 are used to encode lower and upper GUIDs respectively. 110 or 11110 is followed by each nibble of the GUID excluding the hyphens.
		\item[$\bullet$] Finally the code 11111 is used for encoding binary symbols ranging from ASCII 0 to 31 and ASCII 128 to 255. The prefix code 00 10 00 11111 is used, followed by the number of such binary symbols encoded as explained in "Encoding counts" section. After this each byte is encoded with 8 bits per character.
		\item[$\bullet$] Encoding binary symbols this way is not efficient and is only available to cover the entire character set.
		\item[$\bullet$] The implementation actually tries to optimize encoding binary sequences by trying to identify UTF-8 sequences within binary sequences in order to get a better compression ratio. 
	\end{itemize}
	
	\subsection{Compression of frequently occuring sequences}
	\begin{itemize}
		\item[$\bullet$] Provision for six frequently occuring text sequences is available with Unishox.
		\item[$\bullet$] Depending on the type of text being encoded following sequences have been identified.
	\end{itemize}
	\begin{tabular}{ | l | l |} \hline
		﻿\textbf{Type of text} & ﻿\textbf{Frequently occuring sequences} \\ \hline
		Default (favours all types) & [\textquotedblleft: \textquotedblleft], [\textquotedblleft: ], [$<$/], [=\textquotedblleft], [\textquotedblleft:\textquotedblleft], [://] \\ \hline
		English sentences & [ the ], [ and ], [tion], [ with], [ing], [ment] \\ \hline
		URL & [https://], [www.], [.com], [http://], [.org], [.net] \\ \hline
		JSON & [\textquotedblleft: \textquotedblleft], [\textquotedblleft: ], [\textquotedblleft,], [\}\}\}], [\textquotedblleft:\textquotedblleft], [\}\}] \\ \hline
		HTML & [$<$/], [=\textquotedblleft], [div], [href], [class], [$<$p$>$] \\ \hline
		XML & [$<$/], [=\textquotedblleft], ["$>$ ], [$<$?xml version=\textquotedblleft1.0\textquotedblright], [xmlns:], [://] \\ \hline
	\end{tabular}
	
	\subsection{Redefinition of Horizontal codes and Presets}
	\begin{itemize}
		\item[$\bullet$] The horizontal codes can be redefined to get better compression ratio, depending on composition of the text to be encoded.
		\item[$\bullet$] Several "preset" codes have been identified for achieving better compression ratios for different compositions as below (Codes are for Alpha, Sym, Num, Dict, Delta):
		\item[$\bullet$] For preset 1 (Alpha only) there are no horizontal code required. For encoding upper case symbols, just the switch code followed by the letter code is sufficient. Further, continuous upper case can be accomplished by using two switch codes. Termination of encoding is accomplished by encoding 3 or 4 switch codes continuously depending on whether continuous upper case encoding is active or not.
		\item[$\bullet$] The codes marked x in the table are the sets that are not expected in the text.
	\end{itemize}
	\begin{tabular}{ | l | l | l |} \hline
		﻿\textbf{Preset} & ﻿\textbf{Codes} & \textbf{Frequent Sequences} \\ \hline
		0 Default (favours all types) & 00, 01, 10, 110, 111 & Default \\ \hline
		1 Alpha only & None * & English sentences \\ \hline
		2 Alpha \& Numeric only & 0, x, 1, x, x & English sentences \\ \hline
		3 Alpha, Num \& Sym only & 0, 10, 11, x, x & Default \\ \hline
		4 Alpha, Num \& Sym only (Text) & 0, 10, 11, x, x & English sentences \\ \hline
		5 Favor Alpha & 0, 100, 101, 110, 111 & English sentences \\ \hline
		6 Favor Dictionary & 00, 01, 110, 10, 111 & Default \\ \hline
		7 Favor Symbols & 100, 0, 101, 110, 111 & Default \\ \hline
		8 Favor Umlaut & 100, 101, 110, 111, 0 & Default \\ \hline
		9 No Dictionary & 00, 01, 10, x, 11 & Default \\ \hline
		10 No Unicode & 00, 01, 10, 11, x & Default \\ \hline
		11 No Unicode (Text) & 00, 01, 10, 11, x & English sentences \\ \hline
		12 Favor URL & 00, 01, 10, 110, 111 & URL \\ \hline
		13 Favor JSON & 00, 01, 10, 110, 111 & JSON \\ \hline
		14 Favor JSON No Unicode & 00, 01, 10, 11, x & JSON \\ \hline
		15 Favor XML & 00, 01, 10, 110, 111 & XML \\ \hline
		16 Favor HTML & 00, 01, 10, 110, 111 & HTML \\ \hline
	\end{tabular}
	
	However, the default horizontal codes work fine for most cases.
	
	\section{Applications}
	
	\begin{itemize}
		\item[$\bullet$] Compression for low memory devices such as Arduino and ESP8266
		\item[$\bullet$] Sending messages over Websockets
		\item[$\bullet$] Compression of Chat application text exchange including Emojis
		\item[$\bullet$] Storing compressed text in databases
		\item[$\bullet$] Faster retrieval speed when used as join keys
		\item[$\bullet$] Bandwidth cost saving for messages transferred to and from Cloud infrastructure
		\item[$\bullet$] Storage cost reduction for Cloud databases
		\item[$\bullet$] Some people even use it for obfuscation
	\end{itemize}
	
	\section{Implementation}
	
	According to the above Rules and Frequency table, an implementation has been developed licensed under Apache License 2.0.
	
	Unishox has been hosted on Github and used in several open source projects shown below:
	
	\begin{itemize}
		\item[$\bullet$] Unishox \newline https://github.com/siara-cc/Unishox
		\item[$\bullet$] Unishox for Javascript \newline https://github.com/siara-cc/Unishox\_JS
		\item[$\bullet$] Python bindings for Unishox \newline https://github.com/tweedge/unishox2-py3
		\item[$\bullet$] Unishox 1 ported to Python for Tasmota \newline https://github.com/arendst/Tasmota/tree/development/tools/unishox
		\item[$\bullet$] Unishox Compression Library for Arduino Progmem \newline https://github.com/siara-cc/Unishox\_Arduino\_Progmem\_lib
		\item[$\bullet$] Sqlite3 User Defined Function for Unishox as loadable extension \newline https://github.com/siara-cc/Unishox\_Sqlite\_UDF
		\item[$\bullet$] Sqlite3 Library for ESP32 \newline https://github.com/siara-cc/esp32\_arduino\_sqlite3\_lib
		\item[$\bullet$] Sqlite3 Library for ESP8266 \newline https://github.com/siara-cc/esp\_arduino\_sqlite3\_lib
		\item[$\bullet$] Sqlite3 Library for ESP-IDF \newline https://github.com/siara-cc/esp32-idf-sqlite3
	\end{itemize}
	
	\section{Performance Comparison}
	
	The performance of Unishox was compared with the various implementations already available for short strings and shown in subsequent sections.
	
	\subsection{Comparison with Unicode compression techniques}
	
	\begin{tabular}{ | p{0.5\textwidth} | r | r | r | r |} \hline
		\multirow{2}{*}{\textbf{Language and Text}} & \multirow{2}{*}{\textbf{Size}} & \multicolumn{3}{c|}{\textbf{Compressed size}} \\ \cline{3-5}
		& & \textbf{Unishox} & \textbf{SCSU} & \textbf{BOCU} \\ \hline
		{\fontspec{Arial Unicode MS} English:Beauty is not in the face. Beauty is a light in the heart.} & 58 & 30 & 58 & 58 \\ \hline
		{\fontspec{Arial Unicode MS} Chinese:美是不是在脸上。 美是心中的亮光。} & 49 & 36 & 36 & 37 \\ \hline
		{\fontspec{Arial Unicode MS} Spanish:La belleza no está en la cara. La belleza es una luz en el corazón. } & 69 & 38 & 67 & 71 \\ \hline
		{\fontspec{Arial Unicode MS} Hindi: सुंदरता चेहरे में नहीं है। सौंदर्य हृदय में प्रकाश है।} & 144 & 53 & 55 & 55 \\ \hline
		{\fontspec{Arial Unicode MS} Bengali:সৌন্দর্য মুখে নেই। সৌন্দর্য হৃদয় একটি আলো।} & 117 & 41 & 48 & 47 \\ \hline
		{\fontspec{Arial Unicode MS} Portugese:A beleza não está na cara. A beleza é a luz no coração.} & 60 & 36 & 55 & 63 \\ \hline
		{\fontspec{Arial Unicode MS} Russian:Красота не в лицо. Красота - это свет в сердце.} & 82 & 44 & 48 & 53 \\ \hline
		{\fontspec{Arial Unicode MS} Japanese:美は顔にありません。 美は心の中の光です。} & 61 & 39 & 37 & 45 \\ \hline
		{\fontspec{Arial Unicode MS} Punjabi:ਸੁੰਦਰਤਾ ਚਿਹਰੇ ਵਿੱਚ ਨਹੀਂ ਹੈ. ਸੁੰਦਰਤਾ ਦੇ ਦਿਲ ਵਿਚ ਚਾਨਣ ਹੈ.} & 141 & 51 & 57 & 59 \\ \hline
		{\fontspec{Arial Unicode MS} Marathi:सौंदर्य चेहरा नाही. सौंदर्य हे हृदयातील एक प्रकाश आहे.} & 142 & 52 & 55 & 58 \\ \hline
		{\fontspec{Arial Unicode MS} Telugu:అందం ముఖంలో లేదు. అందం హృదయంలో ఒక కాంతి.} & 104 & 39 & 42 & 44 \\ \hline
		{\fontspec{Arial Unicode MS} Turkish:Güzellik yüzünde değil. Güzellik, kalbin içindeki bir ışıktır.} & 72 & 49 & 64 & 78 \\ \hline
		{\fontspec{Arial Unicode MS} Korean: 아름다움은 얼굴에 없습니다。 아름다움은 마음의 빛입니다。} & 82 & 45 & 61 & 60 \\ \hline
		{\fontspec{Arial Unicode MS} French:La beauté est pas dans le visage. La beauté est la lumière dans le coeur.} & 76 & 39 & 73 & 79 \\ \hline
		{\fontspec{Arial Unicode MS} German:Schönheit ist nicht im Gesicht. Schönheit ist ein Licht im Herzen.} & 68 & 36 & 66 & 70 \\ \hline
		{\fontspec{Arial Unicode MS} Vietnamese:Vẻ đẹp không nằm trong khuôn mặt. Vẻ đẹp là ánh sáng trong tim.} & 82 & 59 & 72 & 83 \\ \hline
		{\fontspec{Arial Unicode MS} Tamil: அழகு முகத்தில் இல்லை. அழகு என்பது இதயத்தின் ஒளி.} & 128 & 49 & 50 & 52 \\ \hline
	\end{tabular}
	* All sizes are in bytes.
	
	The above table compares the compression performance between SCSU, BOCU and Unishox for languages that are spoken by over 75 million people (according to Wikipedia).
	
	Disclaimer: Natives may not consider all translations to be accurate as they were translated online, although some attempt was made to check accuracy by reverse translation.
	
	\subsection{Comparison with non-Unicode compression techniques}
	
	\begin{tabular}{ | p{0.5\textwidth} | r | r | r | r |} \hline
		\textbf{String} & \textbf{Size} & \textbf{Unishox} & \textbf{Smaz} & \textbf{Shoco} \\ \hline
		Beauty is not in the face. Beauty is a light in the heart. & 58 & 30 & 31 & 46 \\ \hline
		The quick brown fox jumps over the lazy dog. & 44 & 31 & 31 & 38 \\ \hline
		WRITING ENTIRELY IN BLOCK CAPITALS IS SHOUTING, and it’s rude & 63 & 49 & 72 & 63 \\ \hline
		Grawlix is a string of typographical symbols (such as \%@\$\&*!) coined in the 1960s & 82 & 60 & 58 & 63 \\ \hline
		Rose is a rose is a rose is a rose. & 35 & 12 & 20 & 25 \\ \hline
		Gravitational Constant (G): 6.67300 x 10\^{}-11 m\^{}3 kg\^{}-1 s\^{}-2 & 59 & 50 & 65 & 51 \\ \hline
		039f7094-83e4-4d7f-aa38-8844c67bd82d & 36 & 18 & 53 & 36 \\ \hline
		2021-07-15T16:37:35.897Z & 24 & 9 & 32 & 24 \\ \hline
		(760) 756-7568 & 14 & 7 & 20 & 14 \\ \hline
		This is a loooooooooooooooooooooong string & 42 & 15 & 32 & 25 \\ \hline
	\end{tabular}
	* All sizes are in bytes.
	
	The above table compares the compression performance of Smaz, shoco and Unishox for different types of strings.
	
	\subsection{Comparison of file compression}
	
	Further - world95.txt - the text file obtained from \emph{The Project Gutenberg Etext of the 1995 CIA World Factbook} was compressed using the three techniques and following are the results:
	
	Original size: 2,988,577 bytes
	
	After Compression using shoco original model: 2,385,934 bytes
	
	After Compression using shoco trained using world95.txt: 2,088,141 bytes
	
	After Compression using Unishox (1024 block size): 1,689,289 bytes
	
	After Compression using Unishox (65536 block size): 1,128,302 bytes
	
	\subsection{Memory requirements}
	
	As for operating memory required, Shoco requires over 2k bytes, smaz requires over 1k. But Unishox requires only around $300$ bytes for compressor and decompressor together, ideal for using it with even Arduino Uno.
	
	\subsection{Speed}
	
	Unishox was found to be the slowest of all since employs several to achieve the best compression. However this should not be too much of an issue in most cases when a single string or few strings are handled at a time.
	
	\section{Conclusion}
	
	As can be seen from the performance numbers, Unishox performs better than available techniques. It can also be seen that it provides optimal compression for text, numbers and special characters in different languages all round.
	
	It is especially useful in memory constrained environments such as embedded devices and sending text messages over websockets to implement Chat bots and applications.
	
	\section{Further work}
	
	It is proposed to make Unishox available in more languages than just C, Javascript and Python, such as Java and C\#.Net. It is also proposed to make it available for more platforms.
	
	\section{Credits}
	
	The author is sincerely thankful to the following people who have notably contributed towards the development of Unishox implementation:
	
	\begin{itemize}
		\item[$\bullet$] Thanks to Jonathan Greenblatt (https://github.com/leafgarden) for his port of Unishox2 that works on Particle Photon (https://github.com/siara-cc/Unishox/tree/master/Arduino)
		\item[$\bullet$] Thanks to Chris Partridge (https://github.com/tweedge) for his port of Unishox2 to CPython and his comprehensive tests using Hypothesis and extensive performance tests (https://github.com/tweedge/unishox2-py3)
		\item[$\bullet$] Thanks to Stephan Hadinger (https://github.com/s-hadinger) for his \href{https://github.com/arendst/Tasmota/tree/development/tools/unishox}{port of Unishox1 to Python for Tasmota}
		\item[$\bullet$] Thanks to Luis Díaz Más (https://github.com/piponazo) for his PRs to support MSVC and CMake setup
		\item[$\bullet$] Thanks to James Z.M. Gao (https://github.com/gsm55) for his PRs on improving presets, safety checks, terminator codes, unit tests, bug fixes, documentation and more.
	\end{itemize}
	
	\section{About the Author}
	
	\emph{Arundale Ramanathan} has over 20 years of experience working in the IT industry. He has worked alternatively in large Corporates, MNCs and Startups, including Viewlocity Asia Pacific Pte. Ltd., IBC Systems Pte. Ltd. and Polaris Software Lab Ltd. He publishes free and open source work at https://github.com/siara-cc. He has a masters degree in Computer Science from Anna University, Chennai, India. He can be reached at arun@siara.cc.
	
	\begin{thebibliography}{1}

		\bibitem{1} Jacobson, G. J (1988). Succinct static data structures (Ph.D. thesis). Pittsburgh, PA: Carnegie Mellon University.

		\bibitem{2} Wikipedia, {\em Succinct data structure}, \href{https://en.wikipedia.org/wiki/Succinct_data_structure}{Succinct data structure at Wikipedia}, updated April 2024.


Jia, Lianyin, Yuna Zhang, Jiaman Ding, Jinguo You, Yinong Chen, and Runxin Li. 2020. "Ext-LOUDS: A Space Efficient Extended LOUDS Index for Superset Query" Applied Sciences 10, no. 23: 8530. https://doi.org/10.3390/app10238530

		\bibitem{1} David MacKay. {\em Information Theory, Inference, and Learning Algorithms}, Cambridge University Press, 2003.
		
		\bibitem{2} Shannon, Claude E. (July 1948). {\em A Mathematical Theory of Communication}, Bell System Technical Journal. 27
		
		\bibitem{3} D. A. Huffman, {\em“A method for the construction of minimum-redundancy codes“}, Proc. IRE, vol. 40, pp. 1098-1101,1952.
		
		\bibitem{4} J. Ziv and A. Lempel. A Universal Algorithm for Data Compression. IEEE Transactions on Information Theory, 23(3):337–343, May 1977.
		
		\bibitem{5} Wikipedia, {\em Delta Encoding}, \href{https://en.wikipedia.org/wiki/Delta\_encoding}{Delta encoding at Wikipedia}, updated July 2019.
		
		\bibitem{6} M. Burrows and D. Wheeler. A Block-Sorting Lossless Data Compression Algorithm. Research Report 124, Digital Equipment Corporation, Palo Alto, CA, USA, May 1994.
		
		\bibitem{7} {\em Statistical Distributions of English Text}. data-compression.com. Archived from the original on 2017-09-18.
		
		\bibitem{8} What is the frequency of the letters of the alphabet in English?, Oxford Dictionary. Oxford University Press. Retrieved 29 December 2012.
		
		\bibitem{9} Wikipedia, {\em Letter frequency}, \href{https://en.wikipedia.org/wiki/Letter\_frequency}{Letter Frequency at Wikipedia}, updated December 2018.
		
		\bibitem{10} Misha et al., {\em A Standard Compression Scheme For Unicode - UTR \#6}, \href{https://www.unicode.org/reports/tr6/tr6-4.html}{TR\#6 at unicode.org}, May 2005.
		
		\bibitem{11} Scherer, Markus W. and Davis, Mark., {\em BOCU-1: MIME-Compatible Unicode Compression}, Unicode Technical Note \#6, version 1 (August 2002). Available online at \href{http://www.unicode.org/notes/tn6/}{TN\#6 at unicode.org}
		
		\bibitem{12} Pavel Studený, Ondřej Holeček, Opera Software ASA, Mark., {\em Fast Compression Algorithm For Unicode Text}, Unicode Technical Note \#31, version 2 (January 2008). \href{http://www.unicode.org/notes/tn31}{TN\#31 at unicode.org}
		
		\bibitem{13} Doug Ewell, {\em A survey of Unicode compression}, Unicode Technical Note \#14, version 1 (January 2004). \href{http://www.unicode.org/notes/tn14/}{TN\#14 at unicode.org}.
		
		\bibitem{14} Salvatore Sanfilippo, {\em SMAZ - compression for very small strings}, \href{https://github.com/antirez/smaz}{Smaz on Github}, February 2012.
		
		\bibitem{15} Christian Schramm, {\em shoco: a fast compressor for short strings}, \href{https://ed-von-schleck.github.io/shoco}{Shoco Homepage}, December 2015.
		
	\end{thebibliography}
	
\end{document}
